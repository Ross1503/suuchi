{
    "docs": [
        {
            "location": "/", 
            "text": "Suuchi\n\n\nHaving inspired from tools like \nUber's Ringpop\n and a strong desire to understand how distributed systems work - Suuchi was born.\n\n\nSuuchi is toolkit to build distributed data systems, that uses \ngRPC\n under the hood as the communication medium. The overall goal of this project is to build pluggable components that can be easily composed by the developer to build a data system of desired characteristics.\n\n\n\n\nThis project is alpha quality and not meant to be used in any production setting. We welcome all kinds of feedback to help improve the library.\n\n\n\n\nCurrent version of Suuchi is \n\"0.2.1\"\n.\n\n\nDependencies\n\n\nMaven\n\n\ndependency\n\n    \ngroupId\nin.ashwanthkumar\n/groupId\n\n    \nartifactId\nsuuchi-core\n/artifactId\n\n    \nversion\n${suuchi.version}\n/version\n\n\n/dependency\n\n\n\n\n\nSBT\n\n\nlibraryDependencies += \nin.ashwanthkumar\n % \nsuuchi-core\n % suuchiVersion\n\n\n\n\nReleases are published to \nSonatype release repository\n that eventually gets mirrored to Maven Central.\n\n\nDevelopment snapshots are available in \nSonatypes's snapshot repository\n.\n\n\nFeatures\n\n\n\n\nEnable partitioning of data using \nConsistent Hashing\n\n\nSupports synchronous replication to desired number of nodes\n\n\nEnables above set of features for any gRPC based service definitions\n\n\n\n\nIf you are a developer looking to use Suuchi, head over to \nQuick Start\n guide to get started.\n\n\nRecipes\n\n\n\n\nDistributed InMemory Database\n\n\nDistributed RocksDB backed KV\n\n\nDistributed KVClient\n\n\n\n\nInternals\n\n\nWe try to document the internal workings of some core pieces of Suuchi for developers interested in contributing or understanding their systems better.\n\n\n\n\nPartitioner\n\n\nReplication\n\n\nRouter\n\n\n\n\nLicense\n\n\nhttps://www.apache.org/licenses/LICENSE-2.0", 
            "title": "Introduction"
        }, 
        {
            "location": "/#suuchi", 
            "text": "Having inspired from tools like  Uber's Ringpop  and a strong desire to understand how distributed systems work - Suuchi was born.  Suuchi is toolkit to build distributed data systems, that uses  gRPC  under the hood as the communication medium. The overall goal of this project is to build pluggable components that can be easily composed by the developer to build a data system of desired characteristics.   This project is alpha quality and not meant to be used in any production setting. We welcome all kinds of feedback to help improve the library.   Current version of Suuchi is  \"0.2.1\" .", 
            "title": "Suuchi"
        }, 
        {
            "location": "/#dependencies", 
            "text": "", 
            "title": "Dependencies"
        }, 
        {
            "location": "/#maven", 
            "text": "dependency \n     groupId in.ashwanthkumar /groupId \n     artifactId suuchi-core /artifactId \n     version ${suuchi.version} /version  /dependency", 
            "title": "Maven"
        }, 
        {
            "location": "/#sbt", 
            "text": "libraryDependencies +=  in.ashwanthkumar  %  suuchi-core  % suuchiVersion  Releases are published to  Sonatype release repository  that eventually gets mirrored to Maven Central.  Development snapshots are available in  Sonatypes's snapshot repository .", 
            "title": "SBT"
        }, 
        {
            "location": "/#features", 
            "text": "Enable partitioning of data using  Consistent Hashing  Supports synchronous replication to desired number of nodes  Enables above set of features for any gRPC based service definitions   If you are a developer looking to use Suuchi, head over to  Quick Start  guide to get started.", 
            "title": "Features"
        }, 
        {
            "location": "/#recipes", 
            "text": "Distributed InMemory Database  Distributed RocksDB backed KV  Distributed KVClient", 
            "title": "Recipes"
        }, 
        {
            "location": "/#internals", 
            "text": "We try to document the internal workings of some core pieces of Suuchi for developers interested in contributing or understanding their systems better.   Partitioner  Replication  Router", 
            "title": "Internals"
        }, 
        {
            "location": "/#license", 
            "text": "https://www.apache.org/licenses/LICENSE-2.0", 
            "title": "License"
        }, 
        {
            "location": "/quick-start/", 
            "text": "Quick Start\n\n\nSuuchi internally uses \ngRPC\n for it's internode communication. If you're new to gRPC, we recommend to follow the tutorial available on \ngrpc.io\n to get started on a basic service.\n\n\nIn the rest of this page we're going to assume you've the following setup\n\n\n\n\n\n\nMaven\n or \nSBT\n based project setup and suuchi dependencies configured already.\n\n\n\n\n\n\nA ProtoBuf based service defined, and gRPC required classes generated.\n\n\n\n\n\n\nYou've also implemented at least one of Services defined in the proto.\n\n\n\n\n\n\nCreate a package \nin.ashwanthkumar.suuchi.getting_started\n\n\n\n\n\n\nCreate a class inside the above package as \nDistributedKV.scala\n, and copy-paste the code-snippet from the \nDistributed Inmemory DB Recipe\n.\n\n\n\n\n\n\nRun the \nDistribuedKV\n object. Once it has started, run the \nDistributedKVClient\n object. This would make RPC calls to the server and get back a response and verify that it's valid.\n\n\n\n\n\n\nThat's it! - you've now built a distributed, partitioned and replicated memory backed KVStore.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/quick-start/#quick-start", 
            "text": "Suuchi internally uses  gRPC  for it's internode communication. If you're new to gRPC, we recommend to follow the tutorial available on  grpc.io  to get started on a basic service.  In the rest of this page we're going to assume you've the following setup    Maven  or  SBT  based project setup and suuchi dependencies configured already.    A ProtoBuf based service defined, and gRPC required classes generated.    You've also implemented at least one of Services defined in the proto.    Create a package  in.ashwanthkumar.suuchi.getting_started    Create a class inside the above package as  DistributedKV.scala , and copy-paste the code-snippet from the  Distributed Inmemory DB Recipe .    Run the  DistribuedKV  object. Once it has started, run the  DistributedKVClient  object. This would make RPC calls to the server and get back a response and verify that it's valid.    That's it! - you've now built a distributed, partitioned and replicated memory backed KVStore.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/internals/partitioner/", 
            "text": "Partitioner\n\n\nTBD", 
            "title": "Partitioner"
        }, 
        {
            "location": "/internals/partitioner/#partitioner", 
            "text": "TBD", 
            "title": "Partitioner"
        }, 
        {
            "location": "/internals/replication/", 
            "text": "Replication Internals\n\n\nSuuchi out of the box comes with \nSynchronous Sequential Replication\n during writes. It's fairly easy to build custom replicators.\n\n\nRefer \n#27\n and \n#23\n on how Replication is implemented.\n\n\nTypes\n\n\n\n\nSynchronous SequentialReplication (default available)\n\n\nSynchronous ParallelReplication - (default available)\n\n\nSynchronous ChainedReplication - (\n#31\n)", 
            "title": "Replication"
        }, 
        {
            "location": "/internals/replication/#replication-internals", 
            "text": "Suuchi out of the box comes with  Synchronous Sequential Replication  during writes. It's fairly easy to build custom replicators.  Refer  #27  and  #23  on how Replication is implemented.", 
            "title": "Replication Internals"
        }, 
        {
            "location": "/internals/replication/#types", 
            "text": "Synchronous SequentialReplication (default available)  Synchronous ParallelReplication - (default available)  Synchronous ChainedReplication - ( #31 )", 
            "title": "Types"
        }, 
        {
            "location": "/internals/router/", 
            "text": "HandleOrForward Router\n\n\nHandleOrForward Router is the entry point of a request in your Suuchi based application. It uses a \nRoutingStrategy\n implementation to decide which nodes in the cluster are eligible for handling the current request. It also takes care of forwarding the request to that particular node and returning the response back to the client.\n\n\nSince there isn't any \nSPOC\n (Single Point of Contact) in the system, any node in the cluster can handle or forward any request automatically. This makes the whole operations of the systems very easy. You can setup a load balancer as an entry point to your app\nbacked by all the nodes in the cluster.\n\n\nRefer \n#23\n, \n#11\n and \n#2\n on how HandleOrForward Router is implemented. TBD - Explain with pictures on how it works.\n\n\nRoutingStrategy\n\n\nRoutingStrategy forms the heart of HandleOrForward router. Out of the box Suuchi comes with the following routing strategies\n\n\n\n\nConsistentHashingRouting\n\n\n\n\nCustom Routers\n\n\nRoutingStrategy\n trait is defined as follows\n\n\ntrait RoutingStrategy {\n  /**\n   * Decides if the incoming message should be forwarded or handled by the current node.\n   *\n   * @tparam ReqT Type of the input Message\n   * @return  Some(MemberAddress) - if the request is meant to be forwarded\n   *          \np\n None - if the request can be handled by the current node\n   */\n  def route[ReqT]: PartialFunction[ReqT, Option[MemberAddress]]\n}\n\n\n\n\nAny implementations of that trait can be passed to HandleOrForward Router.\n\n\nNotes\n\n\n\n\nHandleOrForward Router is implemented internally as a ServerInterceptor. What this means is, when you're handling a streaming request every message that's sent in the stream goes through HandleOrForward backed by a RoutingStrategy to decide which nodes the request should go to.", 
            "title": "Router"
        }, 
        {
            "location": "/internals/router/#handleorforward-router", 
            "text": "HandleOrForward Router is the entry point of a request in your Suuchi based application. It uses a  RoutingStrategy  implementation to decide which nodes in the cluster are eligible for handling the current request. It also takes care of forwarding the request to that particular node and returning the response back to the client.  Since there isn't any  SPOC  (Single Point of Contact) in the system, any node in the cluster can handle or forward any request automatically. This makes the whole operations of the systems very easy. You can setup a load balancer as an entry point to your app\nbacked by all the nodes in the cluster.  Refer  #23 ,  #11  and  #2  on how HandleOrForward Router is implemented. TBD - Explain with pictures on how it works.", 
            "title": "HandleOrForward Router"
        }, 
        {
            "location": "/internals/router/#routingstrategy", 
            "text": "RoutingStrategy forms the heart of HandleOrForward router. Out of the box Suuchi comes with the following routing strategies   ConsistentHashingRouting", 
            "title": "RoutingStrategy"
        }, 
        {
            "location": "/internals/router/#custom-routers", 
            "text": "RoutingStrategy  trait is defined as follows  trait RoutingStrategy {\n  /**\n   * Decides if the incoming message should be forwarded or handled by the current node.\n   *\n   * @tparam ReqT Type of the input Message\n   * @return  Some(MemberAddress) - if the request is meant to be forwarded\n   *           p  None - if the request can be handled by the current node\n   */\n  def route[ReqT]: PartialFunction[ReqT, Option[MemberAddress]]\n}  Any implementations of that trait can be passed to HandleOrForward Router.", 
            "title": "Custom Routers"
        }, 
        {
            "location": "/internals/router/#notes", 
            "text": "HandleOrForward Router is implemented internally as a ServerInterceptor. What this means is, when you're handling a streaming request every message that's sent in the stream goes through HandleOrForward backed by a RoutingStrategy to decide which nodes the request should go to.", 
            "title": "Notes"
        }, 
        {
            "location": "/recipes/inmemorydb/", 
            "text": "Distributed InMemory Database\n\n\nFollowing code builds a consistent hashing based Get/Put requests backed by an ConcurrentHashMap.\n\n\npackage in.ashwanthkumar.suuchi\n\nimport java.nio.ByteBuffer\n\nimport in.ashwanthkumar.suuchi.client.SuuchiClient\nimport in.ashwanthkumar.suuchi.router.ConsistentHashingRouting\nimport in.ashwanthkumar.suuchi.rpc.Server.whoami\nimport in.ashwanthkumar.suuchi.rpc.{Server, SuuchiPutService, SuuchiReadService}\nimport in.ashwanthkumar.suuchi.store.InMemoryStore\nimport io.grpc.netty.NettyServerBuilder\n\nobject DistributedKVServer extends App {\n  val REPLICATION_FACTOR = 2\n  val routingStrategy = ConsistentHashingRouting(REPLICATION_FACTOR, whoami(5051), whoami(5052), whoami(5053))\n\n  val store1 = new InMemoryStore\n  val server1 = Server(NettyServerBuilder.forPort(5051), whoami(5051))\n    .routeUsing(new SuuchiReadService(store1), routingStrategy)\n    .withReplication(new SuuchiPutService(store1), REPLICATION_FACTOR, routingStrategy)\n  server1.start()\n\n  val store2 = new InMemoryStore\n  val server2 = Server(NettyServerBuilder.forPort(5052), whoami(5052))\n    .routeUsing(new SuuchiReadService(store2), routingStrategy)\n    .withReplication(new SuuchiPutService(store2), REPLICATION_FACTOR, routingStrategy)\n  server2.start()\n\n  val store3 = new InMemoryStore\n  val server3 = Server(NettyServerBuilder.forPort(5053), whoami(5053))\n    .routeUsing(new SuuchiReadService(store3), routingStrategy)\n    .withReplication(new SuuchiPutService(store3), REPLICATION_FACTOR, routingStrategy)\n  server3.start()\n\n  server1.blockUntilShutdown()\n  server2.blockUntilShutdown()\n  server3.blockUntilShutdown()\n}\n\n\n\n\nLet's break down the above code step by step.\n\n\n\n\nConsistentHashingRouting\n is a \nRouting Strategy\n that does routing between all the nodes using a ConsistentHashRing underneath with default vnode factor of 3.\n\n\nNettyServerBuilder.forPort(5051)\n creates a NettyServer on \n5051\n port.\n\n\nserver.routeUsing()\n adds a new protobuf rpc using a custom routing strategy behind \nHandleOrForward\n router.\n\n\nserver.withReplication()\n adds a new protobuf rpc using the ReplicationRouter. By default it wraps both \nHandleOrForward\n and \nReplicator\n routers.\n\n\nserver1.start()\n starts the underlying gRPC server.\n\n\nserver1.blockUntilShutdown()\n waits until the server is stopped.\n\n\n\n\nTo see this recipe in action, you might also want to look into the client which can talk to this service - \nDistributedKVClient\n.", 
            "title": "Distributed In Memory Database"
        }, 
        {
            "location": "/recipes/inmemorydb/#distributed-inmemory-database", 
            "text": "Following code builds a consistent hashing based Get/Put requests backed by an ConcurrentHashMap.  package in.ashwanthkumar.suuchi\n\nimport java.nio.ByteBuffer\n\nimport in.ashwanthkumar.suuchi.client.SuuchiClient\nimport in.ashwanthkumar.suuchi.router.ConsistentHashingRouting\nimport in.ashwanthkumar.suuchi.rpc.Server.whoami\nimport in.ashwanthkumar.suuchi.rpc.{Server, SuuchiPutService, SuuchiReadService}\nimport in.ashwanthkumar.suuchi.store.InMemoryStore\nimport io.grpc.netty.NettyServerBuilder\n\nobject DistributedKVServer extends App {\n  val REPLICATION_FACTOR = 2\n  val routingStrategy = ConsistentHashingRouting(REPLICATION_FACTOR, whoami(5051), whoami(5052), whoami(5053))\n\n  val store1 = new InMemoryStore\n  val server1 = Server(NettyServerBuilder.forPort(5051), whoami(5051))\n    .routeUsing(new SuuchiReadService(store1), routingStrategy)\n    .withReplication(new SuuchiPutService(store1), REPLICATION_FACTOR, routingStrategy)\n  server1.start()\n\n  val store2 = new InMemoryStore\n  val server2 = Server(NettyServerBuilder.forPort(5052), whoami(5052))\n    .routeUsing(new SuuchiReadService(store2), routingStrategy)\n    .withReplication(new SuuchiPutService(store2), REPLICATION_FACTOR, routingStrategy)\n  server2.start()\n\n  val store3 = new InMemoryStore\n  val server3 = Server(NettyServerBuilder.forPort(5053), whoami(5053))\n    .routeUsing(new SuuchiReadService(store3), routingStrategy)\n    .withReplication(new SuuchiPutService(store3), REPLICATION_FACTOR, routingStrategy)\n  server3.start()\n\n  server1.blockUntilShutdown()\n  server2.blockUntilShutdown()\n  server3.blockUntilShutdown()\n}  Let's break down the above code step by step.   ConsistentHashingRouting  is a  Routing Strategy  that does routing between all the nodes using a ConsistentHashRing underneath with default vnode factor of 3.  NettyServerBuilder.forPort(5051)  creates a NettyServer on  5051  port.  server.routeUsing()  adds a new protobuf rpc using a custom routing strategy behind  HandleOrForward  router.  server.withReplication()  adds a new protobuf rpc using the ReplicationRouter. By default it wraps both  HandleOrForward  and  Replicator  routers.  server1.start()  starts the underlying gRPC server.  server1.blockUntilShutdown()  waits until the server is stopped.   To see this recipe in action, you might also want to look into the client which can talk to this service -  DistributedKVClient .", 
            "title": "Distributed InMemory Database"
        }, 
        {
            "location": "/recipes/rocksdb/", 
            "text": "Distributed RocksDB backed KV\n\n\nDependencies\n\n\ndependency\n\n    \ngroupId\nin.ashwanthkumar\n/groupId\n\n    \nartifactId\nsuuchi-core\n/artifactId\n\n    \nversion\n${suuchi.version}\n/version\n\n\n/dependency\n\n\ndependency\n\n    \ngroupId\nin.ashwanthkumar\n/groupId\n\n    \nartifactId\nsuuchi-rocksdb\n/artifactId\n\n    \nversion\n${suuchi.version}\n/version\n\n\n/dependency\n\n\n\n\n\nCode\n\n\nFollowing code builds a consistent hashing based Get/Put requests backed by \nRocksDB\n. It also does replication for Put requests to \nREPLICATION_COUNT\n number of nodes in the cluster.\n\n\npackage in.ashwanthkumar.suuchi.example\n\nimport java.nio.file.Files\n\nimport in.ashwanthkumar.suuchi.router.ConsistentHashingRouting\nimport in.ashwanthkumar.suuchi.rpc.Server._\nimport in.ashwanthkumar.suuchi.rpc.{Server, SuuchiPutService, SuuchiReadService}\nimport in.ashwanthkumar.suuchi.store.rocksdb.{RocksDbConfiguration, RocksDbStore}\nimport io.grpc.netty.NettyServerBuilder\n\nobject DistributedRocksDb extends App {\n  val REPLICATION_COUNT = 2\n  val routingStrategy = ConsistentHashingRouting(REPLICATION_COUNT, whoami(5051), whoami(5052))\n\n  val path1 = Files.createTempDirectory(\ndistributed-rocksdb\n).toFile\n  val store1 = new RocksDbStore(RocksDbConfiguration(path1.getAbsolutePath))\n  val server1 = Server(NettyServerBuilder.forPort(5051), whoami(5051))\n    .routeUsing(new SuuchiReadService(store1), routingStrategy)\n    .withReplication(new SuuchiPutService(store1), REPLICATION_COUNT, routingStrategy)\n  server1.start()\n\n  val path2 = Files.createTempDirectory(\ndistributed-rocksdb\n).toFile\n  val store2 = new RocksDbStore(RocksDbConfiguration(path2.getAbsolutePath))\n  val server2 = Server(NettyServerBuilder.forPort(5052), whoami(5052))\n    .routeUsing(new SuuchiReadService(store2), routingStrategy)\n    .withReplication(new SuuchiPutService(store2), REPLICATION_COUNT, routingStrategy)\n  server2.start()\n\n  server1.blockUntilShutdown()\n  server2.blockUntilShutdown()\n\n  /*\n    Optionally if want to delete the rocksdb directory\n      path1.delete()\n      path2.delete()\n  */\n}\n\n\n\n\nThis code is available as part of \nsuuchi-examples\n module in the repo.\n\n\nTo see this recipe in action, you might also want to look into the client which can talk to this service - \nDistributedKVClient\n.", 
            "title": "Distributed RocksDB Database"
        }, 
        {
            "location": "/recipes/rocksdb/#distributed-rocksdb-backed-kv", 
            "text": "", 
            "title": "Distributed RocksDB backed KV"
        }, 
        {
            "location": "/recipes/rocksdb/#dependencies", 
            "text": "dependency \n     groupId in.ashwanthkumar /groupId \n     artifactId suuchi-core /artifactId \n     version ${suuchi.version} /version  /dependency  dependency \n     groupId in.ashwanthkumar /groupId \n     artifactId suuchi-rocksdb /artifactId \n     version ${suuchi.version} /version  /dependency", 
            "title": "Dependencies"
        }, 
        {
            "location": "/recipes/rocksdb/#code", 
            "text": "Following code builds a consistent hashing based Get/Put requests backed by  RocksDB . It also does replication for Put requests to  REPLICATION_COUNT  number of nodes in the cluster.  package in.ashwanthkumar.suuchi.example\n\nimport java.nio.file.Files\n\nimport in.ashwanthkumar.suuchi.router.ConsistentHashingRouting\nimport in.ashwanthkumar.suuchi.rpc.Server._\nimport in.ashwanthkumar.suuchi.rpc.{Server, SuuchiPutService, SuuchiReadService}\nimport in.ashwanthkumar.suuchi.store.rocksdb.{RocksDbConfiguration, RocksDbStore}\nimport io.grpc.netty.NettyServerBuilder\n\nobject DistributedRocksDb extends App {\n  val REPLICATION_COUNT = 2\n  val routingStrategy = ConsistentHashingRouting(REPLICATION_COUNT, whoami(5051), whoami(5052))\n\n  val path1 = Files.createTempDirectory( distributed-rocksdb ).toFile\n  val store1 = new RocksDbStore(RocksDbConfiguration(path1.getAbsolutePath))\n  val server1 = Server(NettyServerBuilder.forPort(5051), whoami(5051))\n    .routeUsing(new SuuchiReadService(store1), routingStrategy)\n    .withReplication(new SuuchiPutService(store1), REPLICATION_COUNT, routingStrategy)\n  server1.start()\n\n  val path2 = Files.createTempDirectory( distributed-rocksdb ).toFile\n  val store2 = new RocksDbStore(RocksDbConfiguration(path2.getAbsolutePath))\n  val server2 = Server(NettyServerBuilder.forPort(5052), whoami(5052))\n    .routeUsing(new SuuchiReadService(store2), routingStrategy)\n    .withReplication(new SuuchiPutService(store2), REPLICATION_COUNT, routingStrategy)\n  server2.start()\n\n  server1.blockUntilShutdown()\n  server2.blockUntilShutdown()\n\n  /*\n    Optionally if want to delete the rocksdb directory\n      path1.delete()\n      path2.delete()\n  */\n}  This code is available as part of  suuchi-examples  module in the repo.  To see this recipe in action, you might also want to look into the client which can talk to this service -  DistributedKVClient .", 
            "title": "Code"
        }, 
        {
            "location": "/recipes/kvclient/", 
            "text": "Distributed KVClient\n\n\nIn the either of \nInMemory KV Store\n or \nRocksDB based KV Store\n recipes we only started a server. We need some way to access the server. This recipe is about a simple gRPC client that does a PUT followed by a GET of the same key in the cluster and verify that they're the same.\n\n\npackage in.ashwanthkumar.suuchi\n\nimport java.nio.ByteBuffer\n\nimport in.ashwanthkumar.suuchi.client.SuuchiClient\n\nobject DistributedKVClient extends App {\n  val client = new SuuchiClient(\nlocalhost\n, 5051)\n  val putResponse = client.put(Array(65.toByte), Array(65.toByte)) // puts k=v as A=A (in bytes)\n  require(putResponse, \nclient should have responded successfully\n)\n\n  val getResponse = client.get(Array(65.toByte))\n  require(getResponse.isDefined, \nserver should return a valid response\n) // gets k=A\n  require(ByteBuffer.wrap(getResponse.get) == ByteBuffer.wrap(Array(65.toByte)), \nresponse seems invalid - it should be A\n)\n\n  println(\nClient has been validated\n)\n}\n\n\n\n\nWhile running with this client you can find information about how the node in the point of contact with the client automatically\n- forwarded requests to the right node based on Input Key and ConsistentHash Ring\n- replicates the given message across multiple nodes again based on ConsistentHashing Ring.", 
            "title": "Distributed KVClient"
        }, 
        {
            "location": "/recipes/kvclient/#distributed-kvclient", 
            "text": "In the either of  InMemory KV Store  or  RocksDB based KV Store  recipes we only started a server. We need some way to access the server. This recipe is about a simple gRPC client that does a PUT followed by a GET of the same key in the cluster and verify that they're the same.  package in.ashwanthkumar.suuchi\n\nimport java.nio.ByteBuffer\n\nimport in.ashwanthkumar.suuchi.client.SuuchiClient\n\nobject DistributedKVClient extends App {\n  val client = new SuuchiClient( localhost , 5051)\n  val putResponse = client.put(Array(65.toByte), Array(65.toByte)) // puts k=v as A=A (in bytes)\n  require(putResponse,  client should have responded successfully )\n\n  val getResponse = client.get(Array(65.toByte))\n  require(getResponse.isDefined,  server should return a valid response ) // gets k=A\n  require(ByteBuffer.wrap(getResponse.get) == ByteBuffer.wrap(Array(65.toByte)),  response seems invalid - it should be A )\n\n  println( Client has been validated )\n}  While running with this client you can find information about how the node in the point of contact with the client automatically\n- forwarded requests to the right node based on Input Key and ConsistentHash Ring\n- replicates the given message across multiple nodes again based on ConsistentHashing Ring.", 
            "title": "Distributed KVClient"
        }
    ]
}