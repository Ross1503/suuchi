{
    "docs": [
        {
            "location": "/", 
            "text": "Suuchi\n\n\nHaving inspired from tools like \nUber's Ringpop\n and a strong desire to understand how distributed systems work - Suuchi was born.\n\n\nSuuchi is toolkit to build distributed data systems, that uses \ngRPC\n under the hood as the communication medium. The overall goal of this project is to build pluggable components that can be easily composed by the developer to build a data system of desired characteristics.\n\n\n\n\nThis project is in beta quality and it's currently running couple of systems in production setting \n@indix\n. We welcome all kinds of feedback to help improve the library.\n\n\n\n\nLatest versions\n\n\n\n\n\n\n\n\n\n\nDependencies\n\n\nMaven\n\n\ndependency\n\n    \ngroupId\nin.ashwanthkumar\n/groupId\n\n    \nartifactId\nsuuchi-core\n/artifactId\n\n    \nversion\n${suuchi.version}\n/version\n\n\n/dependency\n\n\n\n\n\nSBT\n\n\nlibraryDependencies += \nin.ashwanthkumar\n % \nsuuchi-core\n % suuchiVersion\n\n\n\n\nReleases are published to \nSonatype release repository\n that eventually gets mirrored to Maven Central.\n\n\nDevelopment snapshots are available in \nSonatypes's snapshot repository\n.\n\n\nFeatures\n\n\n\n\nEnable partitioning of data using \nConsistent Hashing\n\n\nSupports synchronous replication to desired number of nodes\n\n\nSupports Reduce - Re-Reduce style \naggregation\n for methods that can be expressed using a \nSemiGroup\n.\n\n\nEnables above set of features for any gRPC based service definitions\n\n\n\n\nIf you are a developer looking to use Suuchi, head over to \nQuick Start\n guide to get started.\n\n\nRecipes\n\n\n\n\nDistributed InMemory Database\n\n\nDistributed RocksDB backed KV\n\n\nDistributed KVClient\n\n\n\n\nInternals\n\n\nWe try to document the internal workings of some core pieces of Suuchi for developers interested in contributing or understanding their systems better.\n\n\n\n\nPartitioner\n\n\nReplication\n\n\nRouter\n\n\n\n\nPresentations\n\n\nFollowing presentations / videos explain motivation behind Suuchi\n\n\n\n\nSuuchi - Distributed Data Systems Toolkit\n\n\nSuuchi - Application Layer Sharding\n\n\nSuuchi - Distributed Systems Primitives\n\n\n\n\nLicense\n\n\nhttps://www.apache.org/licenses/LICENSE-2.0", 
            "title": "Introduction"
        }, 
        {
            "location": "/#suuchi", 
            "text": "Having inspired from tools like  Uber's Ringpop  and a strong desire to understand how distributed systems work - Suuchi was born.  Suuchi is toolkit to build distributed data systems, that uses  gRPC  under the hood as the communication medium. The overall goal of this project is to build pluggable components that can be easily composed by the developer to build a data system of desired characteristics.   This project is in beta quality and it's currently running couple of systems in production setting  @indix . We welcome all kinds of feedback to help improve the library.", 
            "title": "Suuchi"
        }, 
        {
            "location": "/#latest-versions", 
            "text": "", 
            "title": "Latest versions"
        }, 
        {
            "location": "/#dependencies", 
            "text": "", 
            "title": "Dependencies"
        }, 
        {
            "location": "/#maven", 
            "text": "dependency \n     groupId in.ashwanthkumar /groupId \n     artifactId suuchi-core /artifactId \n     version ${suuchi.version} /version  /dependency", 
            "title": "Maven"
        }, 
        {
            "location": "/#sbt", 
            "text": "libraryDependencies +=  in.ashwanthkumar  %  suuchi-core  % suuchiVersion  Releases are published to  Sonatype release repository  that eventually gets mirrored to Maven Central.  Development snapshots are available in  Sonatypes's snapshot repository .", 
            "title": "SBT"
        }, 
        {
            "location": "/#features", 
            "text": "Enable partitioning of data using  Consistent Hashing  Supports synchronous replication to desired number of nodes  Supports Reduce - Re-Reduce style  aggregation  for methods that can be expressed using a  SemiGroup .  Enables above set of features for any gRPC based service definitions   If you are a developer looking to use Suuchi, head over to  Quick Start  guide to get started.", 
            "title": "Features"
        }, 
        {
            "location": "/#recipes", 
            "text": "Distributed InMemory Database  Distributed RocksDB backed KV  Distributed KVClient", 
            "title": "Recipes"
        }, 
        {
            "location": "/#internals", 
            "text": "We try to document the internal workings of some core pieces of Suuchi for developers interested in contributing or understanding their systems better.   Partitioner  Replication  Router", 
            "title": "Internals"
        }, 
        {
            "location": "/#presentations", 
            "text": "Following presentations / videos explain motivation behind Suuchi   Suuchi - Distributed Data Systems Toolkit  Suuchi - Application Layer Sharding  Suuchi - Distributed Systems Primitives", 
            "title": "Presentations"
        }, 
        {
            "location": "/#license", 
            "text": "https://www.apache.org/licenses/LICENSE-2.0", 
            "title": "License"
        }, 
        {
            "location": "/quick-start/", 
            "text": "Quick Start\n\n\n\n\n\n\nClone the repository \nhttps://github.com/ashwanthkumar/suuchi-getting-started\n on your local machine.\n\n\n\n\n\n\nRun \nmvn clean compile\n to generate the proto stubs for the project.\n\n\n\n\n\n\nImport the project into your favorite IDE.\n\n\n\n\n\n\nCreate 3 Run configurations for \nDistributedKVServer\n main method with different arguments as 5051, 5052 and 5053 and start them all.\n\n\n\n\n\n\nOpen \nSuuchiClient.scala\n and run it to see them in action.\n\n\n\n\n\n\nThat's it! - you've now built a distributed, partitioned and replicated memory backed KVStore.\n\n\n\n\n\n\nSee the Replication in Action\n\n\n\n\n\n\nChange the port from \n5051\n to \n5052\n and stop the 5051 \nDistributedKVServer\n instance.\n\n\n\n\n\n\nRemove the \nclient.put(...)\n from the \nSuuchiClient\n to avoid writes into the cluster.\n\n\n\n\n\n\nNow start the client's main method again, this time the reads should go through fine.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/quick-start/#quick-start", 
            "text": "Clone the repository  https://github.com/ashwanthkumar/suuchi-getting-started  on your local machine.    Run  mvn clean compile  to generate the proto stubs for the project.    Import the project into your favorite IDE.    Create 3 Run configurations for  DistributedKVServer  main method with different arguments as 5051, 5052 and 5053 and start them all.    Open  SuuchiClient.scala  and run it to see them in action.    That's it! - you've now built a distributed, partitioned and replicated memory backed KVStore.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/quick-start/#see-the-replication-in-action", 
            "text": "Change the port from  5051  to  5052  and stop the 5051  DistributedKVServer  instance.    Remove the  client.put(...)  from the  SuuchiClient  to avoid writes into the cluster.    Now start the client's main method again, this time the reads should go through fine.", 
            "title": "See the Replication in Action"
        }, 
        {
            "location": "/internals/aggregation/", 
            "text": "Aggregation\n\n\nAvailable since version 0.2.21 onwards\n\n\nAggregation\n is a special type of router used to perform aggregation type of operation across all the nodes in the cluster.\nIt is used to fan-out requests to all the nodes in the cluster, collect the response and aggregate their the responses. \n\n\nAggregations can be used with all functions that exhibit both associative and commutative property like Sum / Product / \nTopK etc. It's conceptually similar to doing reduce individually on all the nodes and doing a global reduction on those\nreduced results.\n\n\nImplementation Details\n\n\nAggregation in Suuchi makes use of Twitter \nAlgebird's Aggregator\n\nto represent how we can aggregate the results of all service calls. In short Aggregation is presented as a \nPartialFunction[MethodDescriptor[ReqT, RespT], Aggregator[RespT, Any, RespT]]\n.\n\n\nExample of an Aggregation that represent SumOfNumbers can be defined as follows\n\n\nclass SumOfNumbers extends Aggregation {\n  override def aggregator[ReqT, RespT] = {\n    case AggregatorGrpc.METHOD_AGGREGATE =\n new Aggregator[Response, Long, Response] {\n      override def prepare(input: Response): Long = input.getOutput\n      override def semigroup: Semigroup[Long] = LongRing\n      override def present(reduced: Long): Response = Response.newBuilder().setOutput(reduced).build()\n    }.asInstanceOf[Aggregator[RespT, Any, RespT]]\n  }\n}\n\n\n\n\nWe compose this \nAggregation\n with \nServer\n abstraction as follows\n\n\n  Server(...)\n    .aggregate(allNodes, new SuuchiAggregatorService(new SumOfNumbers), new SumOfNumbers)\n    .start()\n\n\n\n\nSuuchiAggregatorService\n filters all even numbers from the store and does a local aggregation of the sum. These sums \nare then globally summed again at the co-ordinator (node that recieves the request for aggregation) node and the result\nis sent back as a response back to the client.\n\n\nDistributed Sum Example\n\n\nLet's consider an example where we would like to find a sum of all even numbers we have on each node. The entire flow\nof data on each node and the co-ordinator node is depicted below in the diagram.\n\n\n\n\nAssume we've 4 nodes Node A - D, and each of them contain a set of numbers with them. The cost of doing filter and sum \non each node if very efficient then returning all the numbers to a single node, filtering them and then computing the sum.\nThis in traditional computer architectural terms is called as function shipping paradigm, very similar to stored procedures \nin RDBMS.", 
            "title": "Aggregation"
        }, 
        {
            "location": "/internals/aggregation/#aggregation", 
            "text": "Available since version 0.2.21 onwards  Aggregation  is a special type of router used to perform aggregation type of operation across all the nodes in the cluster.\nIt is used to fan-out requests to all the nodes in the cluster, collect the response and aggregate their the responses.   Aggregations can be used with all functions that exhibit both associative and commutative property like Sum / Product / \nTopK etc. It's conceptually similar to doing reduce individually on all the nodes and doing a global reduction on those\nreduced results.", 
            "title": "Aggregation"
        }, 
        {
            "location": "/internals/aggregation/#implementation-details", 
            "text": "Aggregation in Suuchi makes use of Twitter  Algebird's Aggregator \nto represent how we can aggregate the results of all service calls. In short Aggregation is presented as a  PartialFunction[MethodDescriptor[ReqT, RespT], Aggregator[RespT, Any, RespT]] .  Example of an Aggregation that represent SumOfNumbers can be defined as follows  class SumOfNumbers extends Aggregation {\n  override def aggregator[ReqT, RespT] = {\n    case AggregatorGrpc.METHOD_AGGREGATE =  new Aggregator[Response, Long, Response] {\n      override def prepare(input: Response): Long = input.getOutput\n      override def semigroup: Semigroup[Long] = LongRing\n      override def present(reduced: Long): Response = Response.newBuilder().setOutput(reduced).build()\n    }.asInstanceOf[Aggregator[RespT, Any, RespT]]\n  }\n}  We compose this  Aggregation  with  Server  abstraction as follows    Server(...)\n    .aggregate(allNodes, new SuuchiAggregatorService(new SumOfNumbers), new SumOfNumbers)\n    .start()  SuuchiAggregatorService  filters all even numbers from the store and does a local aggregation of the sum. These sums \nare then globally summed again at the co-ordinator (node that recieves the request for aggregation) node and the result\nis sent back as a response back to the client.", 
            "title": "Implementation Details"
        }, 
        {
            "location": "/internals/aggregation/#distributed-sum-example", 
            "text": "Let's consider an example where we would like to find a sum of all even numbers we have on each node. The entire flow\nof data on each node and the co-ordinator node is depicted below in the diagram.   Assume we've 4 nodes Node A - D, and each of them contain a set of numbers with them. The cost of doing filter and sum \non each node if very efficient then returning all the numbers to a single node, filtering them and then computing the sum.\nThis in traditional computer architectural terms is called as function shipping paradigm, very similar to stored procedures \nin RDBMS.", 
            "title": "Distributed Sum Example"
        }, 
        {
            "location": "/internals/partitioner/", 
            "text": "Partitioner\n\n\nPartitioners are defined by the following\n\n\ntrait Partitioner {\n  def find(key: Array[Byte], replicaCount: Int): List[MemberAddress]\n}\n\n\n\n\nAn implementation of Partitioner is supposed to return the list of nodes where the given key should be placed if we need \nreplicaCount\n number of replicas. \n\n\nSuuchi by default comes with a ConsistentHashPartitioner which uses ConsistentHashRing underneath to partition the data.\n\n\nInteresting readings on Consistent Hash Ring\n\n\n\n\nhttp://blog.plasmaconduit.com/consistent-hashing/\n\n\nhttp://www.paperplanes.de/2011/12/9/the-magic-of-consistent-hashing.html\n\n\n\n\nAn example of CH Ring during assignment or replication.", 
            "title": "Partitioner"
        }, 
        {
            "location": "/internals/partitioner/#partitioner", 
            "text": "Partitioners are defined by the following  trait Partitioner {\n  def find(key: Array[Byte], replicaCount: Int): List[MemberAddress]\n}  An implementation of Partitioner is supposed to return the list of nodes where the given key should be placed if we need  replicaCount  number of replicas.   Suuchi by default comes with a ConsistentHashPartitioner which uses ConsistentHashRing underneath to partition the data.  Interesting readings on Consistent Hash Ring   http://blog.plasmaconduit.com/consistent-hashing/  http://www.paperplanes.de/2011/12/9/the-magic-of-consistent-hashing.html   An example of CH Ring during assignment or replication.", 
            "title": "Partitioner"
        }, 
        {
            "location": "/internals/replication/", 
            "text": "Replication Internals\n\n\nSuuchi out of the box comes with \nSynchronous Sequential Replication\n during writes. It's fairly easy to build custom replicators.\n\n\nRefer \n#27\n and \n#23\n on how Replication is implemented.\n\n\nTypes\n\n\n\n\nSynchronous SequentialReplication (default available)\n\n\nSynchronous ParallelReplication - (default available)\n\n\nSynchronous ChainedReplication - (\n#31\n)", 
            "title": "Replication"
        }, 
        {
            "location": "/internals/replication/#replication-internals", 
            "text": "Suuchi out of the box comes with  Synchronous Sequential Replication  during writes. It's fairly easy to build custom replicators.  Refer  #27  and  #23  on how Replication is implemented.", 
            "title": "Replication Internals"
        }, 
        {
            "location": "/internals/replication/#types", 
            "text": "Synchronous SequentialReplication (default available)  Synchronous ParallelReplication - (default available)  Synchronous ChainedReplication - ( #31 )", 
            "title": "Types"
        }, 
        {
            "location": "/internals/router/", 
            "text": "HandleOrForward Router\n\n\nHandleOrForward Router is the entry point of a request in your Suuchi based application. It uses a \nRoutingStrategy\n implementation to decide which nodes in the cluster are eligible for handling the current request. It also takes care of forwarding the request to that particular node and returning the response back to the client.\n\n\nSince there isn't any \nSPOC\n (Single Point of Contact) in the system, any node in the cluster can handle or forward any request automatically. This makes the whole operations of the systems very easy. You can setup a load balancer as an entry point to your app\nbacked by all the nodes in the cluster.\n\n\nRefer \n#23\n, \n#11\n and \n#2\n on how HandleOrForward Router is implemented. TBD - Explain with pictures on how it works.\n\n\nRoutingStrategy\n\n\nRoutingStrategy forms the heart of HandleOrForward router. Out of the box Suuchi comes with the following routing strategies\n\n\n\n\nConsistentHashingRouting\n\n\n\n\nCustom Routers\n\n\nRoutingStrategy\n trait is defined as follows\n\n\ntrait RoutingStrategy {\n  /**\n   * Decides if the incoming message should be forwarded or handled by the current node.\n   *\n   * @tparam ReqT Type of the input Message\n   * @return  Some(MemberAddress) - if the request is meant to be forwarded\n   *          \np\n None - if the request can be handled by the current node\n   */\n  def route[ReqT]: PartialFunction[ReqT, Option[MemberAddress]]\n}\n\n\n\n\nAny implementations of that trait can be passed to HandleOrForward Router.\n\n\nNotes\n\n\n\n\nHandleOrForward Router is implemented internally as a ServerInterceptor. What this means is, when you're handling a streaming request every message that's sent in the stream goes through HandleOrForward backed by a RoutingStrategy to decide which nodes the request should go to.", 
            "title": "Router"
        }, 
        {
            "location": "/internals/router/#handleorforward-router", 
            "text": "HandleOrForward Router is the entry point of a request in your Suuchi based application. It uses a  RoutingStrategy  implementation to decide which nodes in the cluster are eligible for handling the current request. It also takes care of forwarding the request to that particular node and returning the response back to the client.  Since there isn't any  SPOC  (Single Point of Contact) in the system, any node in the cluster can handle or forward any request automatically. This makes the whole operations of the systems very easy. You can setup a load balancer as an entry point to your app\nbacked by all the nodes in the cluster.  Refer  #23 ,  #11  and  #2  on how HandleOrForward Router is implemented. TBD - Explain with pictures on how it works.", 
            "title": "HandleOrForward Router"
        }, 
        {
            "location": "/internals/router/#routingstrategy", 
            "text": "RoutingStrategy forms the heart of HandleOrForward router. Out of the box Suuchi comes with the following routing strategies   ConsistentHashingRouting", 
            "title": "RoutingStrategy"
        }, 
        {
            "location": "/internals/router/#custom-routers", 
            "text": "RoutingStrategy  trait is defined as follows  trait RoutingStrategy {\n  /**\n   * Decides if the incoming message should be forwarded or handled by the current node.\n   *\n   * @tparam ReqT Type of the input Message\n   * @return  Some(MemberAddress) - if the request is meant to be forwarded\n   *           p  None - if the request can be handled by the current node\n   */\n  def route[ReqT]: PartialFunction[ReqT, Option[MemberAddress]]\n}  Any implementations of that trait can be passed to HandleOrForward Router.", 
            "title": "Custom Routers"
        }, 
        {
            "location": "/internals/router/#notes", 
            "text": "HandleOrForward Router is implemented internally as a ServerInterceptor. What this means is, when you're handling a streaming request every message that's sent in the stream goes through HandleOrForward backed by a RoutingStrategy to decide which nodes the request should go to.", 
            "title": "Notes"
        }, 
        {
            "location": "/recipes/inmemorydb/", 
            "text": "Distributed InMemory Database\n\n\nFollowing code builds a consistent hashing based Get/Put requests backed by an ConcurrentHashMap in memory.\n\n\npackage in.ashwanthkumar.suuchi.example\n\nimport java.nio.ByteBuffer\n\nimport in.ashwanthkumar.suuchi.client.SuuchiClient\nimport in.ashwanthkumar.suuchi.router.ConsistentHashingRouting\nimport in.ashwanthkumar.suuchi.rpc.Server.whoami\nimport in.ashwanthkumar.suuchi.rpc.{Server, SuuchiPutService, SuuchiReadService}\nimport in.ashwanthkumar.suuchi.store.InMemoryStore\nimport io.grpc.netty.NettyServerBuilder\n\nobject DistributedKVServer extends App {\n  val port = args(0).toInt\n  val PARTITIONS_PER_NODE = 100\n  val REPLICATION_FACTOR = 2\n\n  val routingStrategy = ConsistentHashingRouting(REPLICATION_FACTOR, PARTITIONS_PER_NODE, whoami(5051), whoami(5052), whoami(5053))\n\n  val store = new InMemoryStore\n  val server = Server(NettyServerBuilder.forPort(port), whoami(port))\n    .routeUsing(new SuuchiReadService(store), routingStrategy)\n    .withParallelReplication(new SuuchiPutService(store), REPLICATION_FACTOR, routingStrategy)\n  server.start()\n\n  server.blockUntilShutdown()\n}\n\n\n\n\nLet's break down the above code step by step.\n\n\n\n\nConsistentHashingRouting\n is a \nRouting Strategy\n that does routing between all the nodes using a ConsistentHashRing underneath with default vnode factor of 3.\n\n\nNettyServerBuilder.forPort(5051)\n creates a NettyServer on \n5051\n port.\n\n\nserver.routeUsing()\n adds a new protobuf rpc using a custom routing strategy behind \nHandleOrForward\n router.\n\n\nserver.withParallelReplication()\n adds a new protobuf rpc using the ReplicationRouter. By default it wraps both \nHandleOrForward\n and \nReplicator\n routers.\n\n\nserver1.start()\n starts the underlying gRPC server.\n\n\nserver1.blockUntilShutdown()\n waits until the server is stopped.\n\n\n\n\nTo see this recipe in action, you might also want to look into the client which can talk to this service - \nDistributedKVClient\n.", 
            "title": "Distributed In Memory Database"
        }, 
        {
            "location": "/recipes/inmemorydb/#distributed-inmemory-database", 
            "text": "Following code builds a consistent hashing based Get/Put requests backed by an ConcurrentHashMap in memory.  package in.ashwanthkumar.suuchi.example\n\nimport java.nio.ByteBuffer\n\nimport in.ashwanthkumar.suuchi.client.SuuchiClient\nimport in.ashwanthkumar.suuchi.router.ConsistentHashingRouting\nimport in.ashwanthkumar.suuchi.rpc.Server.whoami\nimport in.ashwanthkumar.suuchi.rpc.{Server, SuuchiPutService, SuuchiReadService}\nimport in.ashwanthkumar.suuchi.store.InMemoryStore\nimport io.grpc.netty.NettyServerBuilder\n\nobject DistributedKVServer extends App {\n  val port = args(0).toInt\n  val PARTITIONS_PER_NODE = 100\n  val REPLICATION_FACTOR = 2\n\n  val routingStrategy = ConsistentHashingRouting(REPLICATION_FACTOR, PARTITIONS_PER_NODE, whoami(5051), whoami(5052), whoami(5053))\n\n  val store = new InMemoryStore\n  val server = Server(NettyServerBuilder.forPort(port), whoami(port))\n    .routeUsing(new SuuchiReadService(store), routingStrategy)\n    .withParallelReplication(new SuuchiPutService(store), REPLICATION_FACTOR, routingStrategy)\n  server.start()\n\n  server.blockUntilShutdown()\n}  Let's break down the above code step by step.   ConsistentHashingRouting  is a  Routing Strategy  that does routing between all the nodes using a ConsistentHashRing underneath with default vnode factor of 3.  NettyServerBuilder.forPort(5051)  creates a NettyServer on  5051  port.  server.routeUsing()  adds a new protobuf rpc using a custom routing strategy behind  HandleOrForward  router.  server.withParallelReplication()  adds a new protobuf rpc using the ReplicationRouter. By default it wraps both  HandleOrForward  and  Replicator  routers.  server1.start()  starts the underlying gRPC server.  server1.blockUntilShutdown()  waits until the server is stopped.   To see this recipe in action, you might also want to look into the client which can talk to this service -  DistributedKVClient .", 
            "title": "Distributed InMemory Database"
        }, 
        {
            "location": "/recipes/rocksdb/", 
            "text": "Distributed RocksDB backed KV\n\n\nDependencies\n\n\ndependency\n\n    \ngroupId\nin.ashwanthkumar\n/groupId\n\n    \nartifactId\nsuuchi-core\n/artifactId\n\n    \nversion\n${suuchi.version}\n/version\n\n\n/dependency\n\n\ndependency\n\n    \ngroupId\nin.ashwanthkumar\n/groupId\n\n    \nartifactId\nsuuchi-rocksdb\n/artifactId\n\n    \nversion\n${suuchi.version}\n/version\n\n\n/dependency\n\n\n\n\n\nCode\n\n\nFollowing code builds a consistent hashing based Get/Put requests backed by \nRocksDB\n. It also does replication for Put requests to \nREPLICATION_COUNT\n number of nodes in the cluster.\n\n\npackage in.ashwanthkumar.suuchi.example\n\nimport java.nio.file.Files\n\nimport in.ashwanthkumar.suuchi.router.ConsistentHashingRouting\nimport in.ashwanthkumar.suuchi.rpc.Server._\nimport in.ashwanthkumar.suuchi.rpc.{Server, SuuchiPutService, SuuchiReadService}\nimport in.ashwanthkumar.suuchi.store.rocksdb.{RocksDbConfiguration, RocksDbStore}\nimport io.grpc.netty.NettyServerBuilder\n\nobject DistributedRocksDb extends App {\n  val port = args(0).toInt\n\n  val REPLICATION_COUNT = 2\n  val PARTITIONS_PER_NODE = 50\n  val routingStrategy = ConsistentHashingRouting(REPLICATION_COUNT, PARTITIONS_PER_NODE, whoami(5051), whoami(5052))\n\n  val path = Files.createTempDirectory(\ndistributed-rocksdb\n).toFile\n  println(s\nUsing ${path.getAbsolutePath} for RocksDB\n)\n  val store = new RocksDbStore(RocksDbConfiguration(path.getAbsolutePath))\n  val server = Server(NettyServerBuilder.forPort(port), whoami(port))\n    .routeUsing(new SuuchiReadService(store), routingStrategy)\n    .withParallelReplication(new SuuchiPutService(store), REPLICATION_COUNT, routingStrategy)\n  server.start()\n  server.blockUntilShutdown()\n\n}\n\n\n\n\nThis code is available as part of \nsuuchi-examples\n module in the repo.\n\n\nTo see this recipe in action, you might also want to look into the client which can talk to this service - \nDistributedKVClient\n.", 
            "title": "Distributed RocksDB Database"
        }, 
        {
            "location": "/recipes/rocksdb/#distributed-rocksdb-backed-kv", 
            "text": "", 
            "title": "Distributed RocksDB backed KV"
        }, 
        {
            "location": "/recipes/rocksdb/#dependencies", 
            "text": "dependency \n     groupId in.ashwanthkumar /groupId \n     artifactId suuchi-core /artifactId \n     version ${suuchi.version} /version  /dependency  dependency \n     groupId in.ashwanthkumar /groupId \n     artifactId suuchi-rocksdb /artifactId \n     version ${suuchi.version} /version  /dependency", 
            "title": "Dependencies"
        }, 
        {
            "location": "/recipes/rocksdb/#code", 
            "text": "Following code builds a consistent hashing based Get/Put requests backed by  RocksDB . It also does replication for Put requests to  REPLICATION_COUNT  number of nodes in the cluster.  package in.ashwanthkumar.suuchi.example\n\nimport java.nio.file.Files\n\nimport in.ashwanthkumar.suuchi.router.ConsistentHashingRouting\nimport in.ashwanthkumar.suuchi.rpc.Server._\nimport in.ashwanthkumar.suuchi.rpc.{Server, SuuchiPutService, SuuchiReadService}\nimport in.ashwanthkumar.suuchi.store.rocksdb.{RocksDbConfiguration, RocksDbStore}\nimport io.grpc.netty.NettyServerBuilder\n\nobject DistributedRocksDb extends App {\n  val port = args(0).toInt\n\n  val REPLICATION_COUNT = 2\n  val PARTITIONS_PER_NODE = 50\n  val routingStrategy = ConsistentHashingRouting(REPLICATION_COUNT, PARTITIONS_PER_NODE, whoami(5051), whoami(5052))\n\n  val path = Files.createTempDirectory( distributed-rocksdb ).toFile\n  println(s Using ${path.getAbsolutePath} for RocksDB )\n  val store = new RocksDbStore(RocksDbConfiguration(path.getAbsolutePath))\n  val server = Server(NettyServerBuilder.forPort(port), whoami(port))\n    .routeUsing(new SuuchiReadService(store), routingStrategy)\n    .withParallelReplication(new SuuchiPutService(store), REPLICATION_COUNT, routingStrategy)\n  server.start()\n  server.blockUntilShutdown()\n\n}  This code is available as part of  suuchi-examples  module in the repo.  To see this recipe in action, you might also want to look into the client which can talk to this service -  DistributedKVClient .", 
            "title": "Code"
        }, 
        {
            "location": "/recipes/kvclient/", 
            "text": "Distributed KVClient\n\n\nIn the either of \nInMemory KV Store\n or \nRocksDB based KV Store\n recipes we only started a server. We need some way to access the server. This recipe is about a simple gRPC client that does a PUT followed by a GET of the same key in the cluster and verify that they're the same.\n\n\npackage in.ashwanthkumar.suuchi\n\nimport java.nio.ByteBuffer\n\nimport in.ashwanthkumar.suuchi.client.SuuchiClient\n\nobject DistributedKVClient extends App {\n  val client = new SuuchiClient(\nlocalhost\n, 5051)\n  val putResponse = client.put(Array(65.toByte), Array(65.toByte)) // puts k=v as A=A (in bytes)\n  require(putResponse, \nclient should have responded successfully\n)\n\n  val getResponse = client.get(Array(65.toByte))\n  require(getResponse.isDefined, \nserver should return a valid response\n) // gets k=A\n  require(ByteBuffer.wrap(getResponse.get) == ByteBuffer.wrap(Array(65.toByte)), \nresponse seems invalid - it should be A\n)\n\n  println(\nClient has been validated\n)\n}\n\n\n\n\nWhile running with this client you can find information about how the node in the point of contact with the client automatically\n- forwarded requests to the right node based on Input Key and ConsistentHash Ring\n- replicates the given message across multiple nodes again based on ConsistentHashing Ring.", 
            "title": "Distributed KVClient"
        }, 
        {
            "location": "/recipes/kvclient/#distributed-kvclient", 
            "text": "In the either of  InMemory KV Store  or  RocksDB based KV Store  recipes we only started a server. We need some way to access the server. This recipe is about a simple gRPC client that does a PUT followed by a GET of the same key in the cluster and verify that they're the same.  package in.ashwanthkumar.suuchi\n\nimport java.nio.ByteBuffer\n\nimport in.ashwanthkumar.suuchi.client.SuuchiClient\n\nobject DistributedKVClient extends App {\n  val client = new SuuchiClient( localhost , 5051)\n  val putResponse = client.put(Array(65.toByte), Array(65.toByte)) // puts k=v as A=A (in bytes)\n  require(putResponse,  client should have responded successfully )\n\n  val getResponse = client.get(Array(65.toByte))\n  require(getResponse.isDefined,  server should return a valid response ) // gets k=A\n  require(ByteBuffer.wrap(getResponse.get) == ByteBuffer.wrap(Array(65.toByte)),  response seems invalid - it should be A )\n\n  println( Client has been validated )\n}  While running with this client you can find information about how the node in the point of contact with the client automatically\n- forwarded requests to the right node based on Input Key and ConsistentHash Ring\n- replicates the given message across multiple nodes again based on ConsistentHashing Ring.", 
            "title": "Distributed KVClient"
        }, 
        {
            "location": "/developer/workflow/", 
            "text": "Release Workflow\n\n\nIn Suuchi and it's related modules we use the following mechanism of doing releases to sonatype.\n\n\nSteps to make a release\n\n\n\n\nMake sure you've write access to the repository.\n\n\nRun the \nmake-release.sh\n from the root of the project. \n\n\nIt would create an empty commit with the message \n\"[Do Release]\"\n.\n\n\nThis commit message would trigger the release workflow using the build tool to build and publish the artifacts to sonatype, which later would get mirrored to maven central.\n\n\n\n\nRelease Process", 
            "title": "Release Workflow"
        }, 
        {
            "location": "/developer/workflow/#release-workflow", 
            "text": "In Suuchi and it's related modules we use the following mechanism of doing releases to sonatype.", 
            "title": "Release Workflow"
        }, 
        {
            "location": "/developer/workflow/#steps-to-make-a-release", 
            "text": "Make sure you've write access to the repository.  Run the  make-release.sh  from the root of the project.   It would create an empty commit with the message  \"[Do Release]\" .  This commit message would trigger the release workflow using the build tool to build and publish the artifacts to sonatype, which later would get mirrored to maven central.", 
            "title": "Steps to make a release"
        }, 
        {
            "location": "/developer/workflow/#release-process", 
            "text": "", 
            "title": "Release Process"
        }
    ]
}